name: Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  TESTING: true
  LOG_LEVEL: DEBUG
  DATABASE_URL: sqlite:///:memory:
  PYTHONPATH: ${{ github.workspace }}

jobs:
  # ============================================================================
  # Code Quality Checks
  # ============================================================================
  lint:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-lint-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-lint-
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort mypy
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi
    
    - name: Run Black (code formatting)
      run: black --check --diff .
    
    - name: Run isort (import sorting)
      run: isort --check-only --diff .
    
    - name: Run flake8 (linting)
      run: flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
    
    - name: Run mypy (type checking)
      run: mypy service/ database/ shared/ --ignore-missing-imports
      continue-on-error: true

  # ============================================================================
  # Unit Tests
  # ============================================================================
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10', '3.11']
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov pytest-asyncio pytest-mock
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        if [ -f requirements-test.txt ]; then pip install -r requirements-test.txt; fi
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ \
          --cov=service \
          --cov=database \
          --cov=shared \
          --cov-report=xml \
          --cov-report=term-missing \
          --cov-fail-under=80 \
          --junitxml=junit/test-results-${{ matrix.python-version }}.xml \
          -v
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unit-tests
        name: codecov-umbrella
        fail_ci_if_error: false
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-unit-${{ matrix.python-version }}
        path: junit/test-results-${{ matrix.python-version }}.xml

  # ============================================================================
  # Integration Tests
  # ============================================================================
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: fraud_detection_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
      
      kafka:
        image: confluentinc/cp-kafka:latest
        env:
          KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
          KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        ports:
          - 9092:9092
        options: >-
          --health-cmd "kafka-topics --bootstrap-server localhost:9092 --list"
          --health-interval 30s
          --health-timeout 10s
          --health-retries 5
      
      zookeeper:
        image: confluentinc/cp-zookeeper:latest
        env:
          ZOOKEEPER_CLIENT_PORT: 2181
          ZOOKEEPER_TICK_TIME: 2000
        ports:
          - 2181:2181
    
    env:
      DATABASE_URL: postgresql://postgres:postgres@localhost:5432/fraud_detection_test
      REDIS_URL: redis://localhost:6379/0
      KAFKA_BOOTSTRAP_SERVERS: localhost:9092
      KAFKA_AVAILABLE: true
      DATABASE_AVAILABLE: true
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-integration-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-integration-
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov pytest-asyncio pytest-mock
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        if [ -f requirements-test.txt ]; then pip install -r requirements-test.txt; fi
    
    - name: Wait for services
      run: |
        # Wait for PostgreSQL
        until pg_isready -h localhost -p 5432; do
          echo "Waiting for PostgreSQL..."
          sleep 2
        done
        
        # Wait for Redis
        until redis-cli -h localhost -p 6379 ping; do
          echo "Waiting for Redis..."
          sleep 2
        done
        
        # Wait for Kafka (simplified check)
        sleep 30
    
    - name: Run database migrations
      run: |
        # Run any database setup scripts here
        echo "Setting up test database..."
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ \
          --cov=service \
          --cov=database \
          --cov-report=xml \
          --cov-report=term-missing \
          --junitxml=junit/test-results-integration.xml \
          -v
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: integration-tests
        name: codecov-umbrella
        fail_ci_if_error: false
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-integration
        path: junit/test-results-integration.xml

  # ============================================================================
  # Performance Tests
  # ============================================================================
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[perf]')
    
    env:
      RUN_SLOW_TESTS: true
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-perf-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-perf-
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-benchmark pytest-asyncio
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        if [ -f requirements-test.txt ]; then pip install -r requirements-test.txt; fi
    
    - name: Run performance tests
      run: |
        pytest tests/performance/ \
          --benchmark-json=benchmark.json \
          --junitxml=junit/test-results-performance.xml \
          -v
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark.json
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-performance
        path: junit/test-results-performance.xml
    
    - name: Comment benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      if: github.event_name == 'push'
      with:
        tool: 'pytest'
        output-file-path: benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        comment-on-alert: true
        alert-threshold: '150%'
        fail-on-alert: false

  # ============================================================================
  # Security Scanning
  # ============================================================================
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit
    
    - name: Run safety check (dependency vulnerabilities)
      run: safety check --json --output safety-report.json
      continue-on-error: true
    
    - name: Run bandit (security linting)
      run: bandit -r service/ database/ shared/ -f json -o bandit-report.json
      continue-on-error: true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          safety-report.json
          bandit-report.json

  # ============================================================================
  # Test Summary
  # ============================================================================
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [lint, unit-tests, integration-tests]
    if: always()
    
    steps:
    - name: Download all test results
      uses: actions/download-artifact@v3
      with:
        path: test-results
    
    - name: Publish test results
      uses: EnricoMi/publish-unit-test-result-action@v2
      if: always()
      with:
        files: "test-results/**/test-results-*.xml"
        check_name: "Test Results"
        comment_title: "Test Results"
        fail_on: "test failures"
    
    - name: Create test summary
      if: always()
      run: |
        echo "## Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Code Quality | ${{ needs.lint.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Unit Tests | ${{ needs.unit-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration Tests | ${{ needs.integration-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [[ "${{ needs.lint.result }}" == "success" && "${{ needs.unit-tests.result }}" == "success" && "${{ needs.integration-tests.result }}" == "success" ]]; then
          echo "✅ All tests passed!" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ Some tests failed. Please check the individual job results." >> $GITHUB_STEP_SUMMARY
        fi

  # ============================================================================
  # Deployment (on successful tests)
  # ============================================================================
  deploy:
    name: Deploy
    runs-on: ubuntu-latest
    needs: [lint, unit-tests, integration-tests]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push' && success()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # Add deployment commands here
    
    - name: Run smoke tests
      run: |
        echo "Running smoke tests on staging..."
        # Add smoke test commands here
    
    - name: Deploy to production
      if: success()
      run: |
        echo "Deploying to production environment..."
        # Add production deployment commands here

  # ============================================================================
  # Notification
  # ============================================================================
  notify:
    name: Notify
    runs-on: ubuntu-latest
    needs: [test-summary]
    if: always() && (github.event_name == 'push' || github.event_name == 'schedule')
    
    steps:
    - name: Notify on success
      if: needs.test-summary.result == 'success'
      run: |
        echo "✅ All tests passed for commit ${{ github.sha }}"
        # Add success notification logic (Slack, email, etc.)
    
    - name: Notify on failure
      if: needs.test-summary.result == 'failure'
      run: |
        echo "❌ Tests failed for commit ${{ github.sha }}"
        # Add failure notification logic (Slack, email, etc.)